# I worked this assignment with 1392146

# importing libraries
import pandas as pd 
import numpy as np
import matplotlib.pyplot as plt  



# Task 1

# A function that simulates data from an AR(1) process 
# Generate AR(2) data              

def ar1(alpha, sigma2, T):
    total_obs = T + 50
    
    sigma = np.sqrt(sigma2)
    
    y = np.zeros(total_obs)
    
    epsilon = np.random.normal(0, sigma, total_obs)
    
    
    for t in range(1, total_obs):
        y[t] = alpha * y[t-1] + epsilon[t]
    
    
    return y[50:]



# Task 2

# Function of MA(1) data
def ma(theta, T):
    ma_data = np.zeros(T)
    wn = np.random.normal(size=T)
    for i in range(1,T):
        ma_data[i] = wn[i] + theta * wn[i-1]
    return ma_data

# Generating data from the MA(1) process, theta=0.7 and T=500  
y = ma(0.7, 500)


# Estimating the autocorrelation 

# importing the built-in mmodule for estimating the autocorrelation
import statsmodels.api as sm
import statsmodels.tsa.stattools as ts

acf = sm.tsa.acf(y, nlags=3)
acf_3 = acf[3]
print(acf_3)


# (i) Conducting the test of null hypothesis being acf_3=0, using the standard error 1/√T

reject = []

for _ in range(0,1000):
    
    #generating the time serires data
    y = ma(0.7, 500)
    
    # calculating the autocorrelation
    acf = sm.tsa.acf(y, nlags=3)
    acf_3 = acf[3] 
    
    # conduncting the test
    if abs(acf_3) >= 2 / np.sqrt(500):
        reject.append(acf_3)
        
print(f"Number of true values: {len(reject)}")
print(f"Number of false values: {1000 - len(reject)}")
print(f"Rejection frequency: {len(reject) / 1000}")


# (ii) Conducting the test of null hypothesis being acf_3=0, using Bartlett's formula for an MA(1) process

reject = []

for _ in range(0,1000):
    
    # generating the time series data
    y = ma(0.7, 500)
    
    # calcuating the autocorrelation 
    acf = sm.tsa.acf(y, nlags=3)
    acf_3 = acf[3] 
    
    # conduncting the test
    if abs(acf_3) >= 2 * np.sqrt(1 + 2 * sm.tsa.acf(y,nlags=1)[1] ** 2) / np.sqrt(500):
        reject.append(acf_3)


print(f"Number of true values: {len(reject)}")
print(f"Number of false values: {1000 - len(reject)}")
print(f"Rejection frequency: {len(reject) / 1000}")



# Interpretation
# The null hypothesis was rejected when the standard error used approximately 9% of the time while it was rejected when Bartlett's formula used approximately 5% of the time.
# The percentage of rejection frequency when the standard error used is more than expected. 
# In conclusion, this result shows us that Bartlett's formula is more precise.



# Task3

# AR(2) function from computer tutorial 1
def ar2gen(alpha1, alpha2, T):            
    ar2_data = np.zeros(T)
    wn_p = np.random.normal(size=T)
    for ii in range(2,T):
        ar2_data[ii] = alpha1*ar2_data[ii-1] + alpha2*ar2_data[ii-2] + \
        wn_p[ii]
    return ar2_data

# Simulate T = 100 observations from AR model
ar2gen(0.6, 0.2, 100)


# Estimate α1 and α2 using the Yule-Walker estimator.

def estimate_ar2_yw(y):
    T = len(y)
    

    gamma = np.zeros(3)
    for k in range(3):
        gamma[k] = np.sum(y[k:] * y[:T-k]) / T
    
    
    R = np.array([[gamma[0], gamma[1]],
                  [gamma[1], gamma[0]]])
    r = np.array([gamma[1], gamma[2]])
    
    params = np.linalg.inv(R).dot(r)
    
    return params

estimate_ar2_yw(ar2gen(0.6, 0.2, 100))


# Repeat the exercise with T = 500 
# and compare your outcome with the previous results
estimate_ar2_yw(ar2gen(0.6, 0.2, 500))

# The longer the data, the more accurate the estimation is expected to be. 
# This is because more data points make the estimation of auto-covariance more accurate.
# As a result, estimates for T=500 tend to be closer to the true parameter values than estimates for T=100



# Task 4

# the coefficients of 1 - 0.5B + 0.3B^2
coefficients = [0.3, -0.5, 1]

# solving the roots 
roots = np.roots(coefficients)
print(roots)


# Task 5

# (a)
time_series = []
for m in range(2000):
    
    time_series.append(ar1(0.4, 1, 200))
    

# (b)
# making the function that calculates the LS estimate (alpha hat)
def ls (time_series):
  """
  calculating the least squares estimation of AR process
  
  time_serires: the time series data is generated by the previous function in (a)
  
  return: the least squares estimation 
  
  """
  result1 = np.zeros((2,2))
  result2 = 0
  
  for i in range(0, 200):
    
    # initializing
      Yt_1 = np.zeros((2,1))
      yt = np.zeros((1,1))
      yt[0][0] = time_series[i]
      
      Yt_1[0][0] = 1
      Yt_1[1][0] = time_series[i-1]
      
      result1 += np.dot(Yt_1, np.transpose(Yt_1))
      result2 += np.dot(Yt_1, yt)
    
  result3 = np.dot(np.linalg.inv(result1), result2)
  return result3[1][0]


# making the function that calculates 1/√T Sigma yt-1 * εt

def statistics1(time_series_data):
    statistics1 = 0
    
    for i in range(0, len(time_series_data)):
        epsilon = time_series_data[i] - 0.4 * time_series_data[i-1]  #rewrite: epsilon = yt - yt-1 * theta
        statistics1 += (epsilon * time_series_data[i-1]) / np.sqrt(len(time_series_data)) #paper of the assisngment
        
    return statistics1 


# loopig 2000 times

# initializing
alpha_hats = []
stat_1s = []
stat_2s = []

# generating the time series data
time_series = []
for _ in range(2000):
    time_series.append(ar1(0.4, 1, 200))

# for looping 2000 times
for i in range(0, 2000):
    ts_data = time_series[i]
    alpha_hats.append(ls(ts_data))
    stat_1s.append(statistics1(ts_data))
    stat_2s.append(np.sqrt(200) * (ls(ts_data) - 0.4))
    

print(alpha_hats)
print(stat_1s)
print(stat_2s)


# (c)
# calculating the bias, which is the difference between alpha hat and alpha

alpha = np.full((200,1), 0.4)

bias = alpha_hats - alpha


# compute the mean bias over the M = 2000 replications
mean_bias = np.mean(bias)
print(f"The mean bias: {mean_bias}")

# plots of the sampling distribution of alpha hat
plt.hist(alpha_hats, bins=30)
plt.title("Sampling distribution of alpha hat for α=0.4")
plt.xlabel("alpha hat")
plt.ylabel("Density")
plt.show()


# (d)
# stat1 (1/√T Sigma yt-1 * εt)
plt.hist(stat_1s, bins=30, label="1/√T Sigma yt-1 * εt", alpha=0.3)

# plot a normal distribution N(0, σˆ4 / (1−αˆ2))
plt.hist(np.random.normal(0,1/(1-(0.4**2)), size=2000), bins=30, alpha=0.5, label="N(0, σˆ4 / (1−αˆ2)")

# show the title and the legend
plt.title("Comparing the sampling distribution of 1/√T Sigma yt-1 * εt with the normal distirbution")
plt.legend()
plt.show()


# stat2 (√T(ˆα − α))
plt.hist(stat_2s, bins = 30, label="√T(ˆα − α)", alpha=0.3)

# plot a normal distribution N(0, 1−αˆ2).
plt.hist(np.random.normal(0,1-(0.4**2), size=2000), bins=30, label="N(0, 1−αˆ2)", alpha=0.5)

# show the titel and the legend
plt.title("Comparing the sampling distribution of √T(ˆα − α) with the normal distirbution")
plt.legend()
plt.show()


# (e)
# alpha = 0.9

# a
# generating time series data
time_series = []
for m in range(2000):
    
    time_series.append(ar1(0.9, 1, 200))
    
# b
# loopig 2000 times
alpha_hats_9 = []
stat_1s_9 = []
stat_2s_9 = []

for i in range(0, 2000):
    ts_data = time_series[i]
    alpha_hats_9.append(ls(ts_data))
    stat_1s_9.append(statistics1(ts_data))
    stat_2s_9.append(np.sqrt(200) * (ls(ts_data) - 0.9))
    
    
# c
# calculating the bias, which is the difference between alpha hat and alpha

alpha = np.full((200,1), 0.9)

bias = alpha_hats - alpha


# compute the mean bias over the M = 2000 replications
mean_bias_9 = np.mean(bias)
print(f"The mean bias: {mean_bias_9}")

# plots of the sampling distribution of alpha hat
plt.hist(alpha_hats_9, bins=30)
plt.title("Sampling distribution of alpha hat for α=0.9")
plt.xlabel("alpha hat")
plt.ylabel("Density")
plt.show()


# d
# stat1 (1/√T Sigma yt-1 * εt)
plt.hist(stat_1s_9, bins=30, label="1/√T Sigma yt-1 * εt", alpha=0.3)

# plot a normal distribution N(0, σˆ4 / (1−αˆ2))
plt.hist(np.random.normal(0,1/(1-(0.4**2)), size=2000), bins=30, alpha=0.5, label="N(0, σˆ4 / (1−αˆ2)")

# show the title and the legend
plt.title("Comparing the sampling distribution of 1/√T Sigma yt-1 * εt with the normal distirbution")
plt.legend()
plt.show()


# stat2 (√T(ˆα − α))
plt.hist(stat_2s_9, bins = 30, label="√T(ˆα − α)", alpha=0.3)

# plot a normal distribution N(0, 1−αˆ2).
plt.hist(np.random.normal(0,1-(0.4**2), size=2000), bins=30, label="N(0, 1−αˆ2)", alpha=0.5)

# show the titel and the legend
plt.title("Comparing the sampling distribution of √T(ˆα − α) with the normal distirbution")
plt.legend()
plt.show()


# alpha = 1

# a
# generating time series data
time_series = []
for m in range(2000):
    
    time_series.append(ar1(1, 1, 200))
    
# b
# loopig 2000 times
alpha_hats_1 = []
stat_1s_1 = []
stat_2s_1 = []

for i in range(0, 2000):
    ts_data = time_series[i]
    alpha_hats_1.append(ls(ts_data))
    stat_1s_1.append(statistics1(ts_data))
    stat_2s_1.append(np.sqrt(200) * (ls(ts_data) - 1))
    
    
# c
# calculating the bias, which is the difference between alpha hat and alpha

alpha = np.full((200,1), 1)

bias = alpha_hats - alpha


# compute the mean bias over the M = 2000 replications
mean_bias_1 = np.mean(bias)
print(f"The mean bias: {mean_bias_1}")

# plots of the sampling distribution of alpha hat
plt.hist(alpha_hats_1, bins=30)
plt.title("Sampling distribution of alpha hat for α=1.0")
plt.xlabel("alpha hat")
plt.ylabel("Density")
plt.show()


# d
# stat1 (1/√T Sigma yt-1 * εt)
plt.hist(stat_1s_1, bins=30, label="1/√T Sigma yt-1 * εt", alpha=0.3)

# plot a normal distribution N(0, σˆ4 / (1−αˆ2))
plt.hist(np.random.normal(0,1/(1-(0.4**2)), size=2000), bins=30, alpha=0.5, label="N(0, σˆ4 / (1−αˆ2)")

# show the title and the legend
plt.title("Comparing the sampling distribution of 1/√T Sigma yt-1 * εt with the normal distirbution")
plt.legend()
plt.show()


# stat2 (√T(ˆα − α))
plt.hist(stat_2s_1, bins = 30, label="√T(ˆα − α)", alpha=0.3)

# plot a normal distribution N(0, 1−αˆ2).
plt.hist(np.random.normal(0,1-(0.4**2), size=2000), bins=30, label="N(0, 1−αˆ2)", alpha=0.5)

# show the titel and the legend
plt.title("Comparing the sampling distribution of √T(ˆα − α) with the normal distirbution")
plt.legend()
plt.show()


# Comparing
#Compare the mean bias and sampling distributions of αˆ with the respective quantities for α = 0.4.
#Also compare the histograms of √T(αˆ − α) for different αs. What do you observe?

# comparing the mean bias 
print(f"The mean bias for α = 0.4: {mean_bias}")
print(f"The mean bias for α = 0.9: {mean_bias_9}")
print(f"The mean bias for α = 1.0: {mean_bias_1}")


# comparing the sampling distribution of alpha hat

plt.hist(alpha_hats, bins=30, label="α=0.4", alpha=0.5)
plt.hist(alpha_hats_9, bins=30, label="α=0.9", alpha=0.5)
plt.hist(alpha_hats_1, bins=30, label="α=1.0", alpha=0.5)

# show the legend 
plt.title("Comparing the sampling distributions of the alpha hat")
plt.legend()
plt.show()


# comparing the histograms of √T(ˆα − α)

plt.hist(stat_2s, bins=30, label="α=0.4", alpha=0.5)
plt.hist(stat_2s_9, bins=30, label="α=0.9", alpha=0.5)
plt.hist(stat_2s_1, bins=30, label="α=1.0", alpha=0.5)

# show the legend 
plt.title("Comparing the histograms of √T(ˆα − α)")
plt.legend()
plt.show()


# What do you observe?
# The estimated alpha values is not over 1. 
# The distributions of the bias converge to the normal distribution for the higher values of alpha and the mean bias increases as the alpha values increas.
# The variance of the distirbution decreases.